{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">IMBD 전문가 평점 - 리뷰 - 박스오피스 수익 크롤러</h1>\n",
    "\n",
    "### 고려대학교 경영대학 : 영화의 대중적 성공과, 전문 비평가의 평가의 관계에 관한 조사\n",
    "#### Dataset :\n",
    "- title | USgross box office revenue | statscore | critic metascores | critic review texts |  \n",
    "- Dataset : [title, US box office revenue, starscore, [(metascores, metatext),(metascores, metatext),(metascores, metatext)...]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 완성된 형태: \n",
    "## < 영화 제목 - 대중 평점 - 박스오피스 수익 - 전문가 평점 - 리뷰 > 순으로 크롤링 됨. \n",
    "## 'IMBD_dataset.csv' 현재 디렉토리에 다운됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Screen%20Shot%202019-06-11%20at%2010.46.22%20AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1. Import all required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2. Store all action movie titles' urls to a variable (bs4, selenium)\n",
    "```whole_title_urls_storage```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import bs4\n",
    "import urllib3\n",
    "import selenium.webdriver as webdriver\n",
    "# !!!! 커스텀 설정 !!!! 너의 크롬 드라이버 파일의 디렉토리\n",
    "from time import sleep\n",
    "from lxml import html\n",
    "import urllib3\n",
    "import csv\n",
    "import os\n",
    "# Get html text and let lxml parse it in its own way.\n",
    "http = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load action genre page\n",
    "driver = webdriver.Chrome('/Users/sucky/Desktop/chromedriver') \n",
    "#action_genre_url = \"https://www.imdb.com/search/title?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=cd28805a-4e91-4f0f-b066-0db5ff4dd1a7&pf_rd_r=30AH67YYPH64GCS9CRA9&pf_rd_s=right-6&pf_rd_t=15506&pf_rd_i=boxoffice&ref_=chtbo_gnr_1&genres=action&explore=title_type,genres\"\n",
    "action_genre_url = 'https://www.imdb.com/search/title?title_type=movie&genres=action&sort=boxoffice_gross_us,desc&explore=title_type,genres'\n",
    "driver.get(action_genre_url)\n",
    "driver.implicitly_wait(1)\n",
    "\n",
    "#Save all the link at issue\n",
    "#1). save all the url links of 253,049 action movies.\n",
    "whole_title_urls_storage = [] # from\n",
    " \n",
    "#2). iterate with selenium to collect all the links\n",
    "\n",
    "for i in range(3): # !!! CUSTOM SETTING !!! : depends on how many movies you want to crawl. (50movies per 1 page)\n",
    "    try :\n",
    "    #wait loading\n",
    "        driver.implicitly_wait(2)\n",
    "    \n",
    "    #get driver's url\n",
    "        now_url = driver.current_url\n",
    "        r = http.request('GET', now_url)\n",
    "        source = r.data\n",
    "        tree = html.document_fromstring(source)\n",
    "    \n",
    "    # get all the href links\n",
    "        one_page_url_list = ['https://www.imdb.com'+i.get('href') for i in tree.cssselect('div div div div div div div div div h3 a')]\n",
    "    \n",
    "    # extend this to whole title urls storage\n",
    "        whole_title_urls_storage.extend(one_page_url_list)\n",
    "    \n",
    "    # click move on to next page\n",
    "        if now_url == action_genre_url:\n",
    "            driver.find_element_by_xpath('//*[@id=\"main\"]/div/div[4]/a').click()\n",
    "        else:\n",
    "            driver.find_element_by_xpath('//*[@id=\"main\"]/div/div[4]/a[2]').click()\n",
    "        \n",
    "    except:\n",
    "        print('error occured at {}th set'.format(i+202))\n",
    "\n",
    "# Speak out when finished.\n",
    "os.system('say \"your program has been completed.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2. - 2) check data size / check if there are any overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(whole_title_urls_storage))) #내가 모은 url이 중복없이 원하는 개수대로 모였는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3. Iterate every url => filter non-movie => access to link to seperate critics' reveiw website => scrape all metascores and matching revies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .csv file\n",
    "f = open('IMBD_dataset.csv', 'w') # !!!! 커스텀 설정 !!!! : 원하는 '디렉터리/파일명.csv'을 첫번째 인풋에.\n",
    "wr = csv.writer(f)\n",
    "\n",
    "# count(url's index)\n",
    "iter = 0\n",
    "\n",
    "# Iterate over collected links\n",
    "for l in whole_title_urls_storage[0:10]:\n",
    "    \n",
    "    # index counter (starts from 1)\n",
    "    iter += 1\n",
    "    print(iter)\n",
    "    \n",
    "    # repeat this\n",
    "    try:\n",
    "       \n",
    "        # <Get html and parsed tree>\n",
    "            # get html of the link\n",
    "        r = http.request('GET', l) # l is one link from loop\n",
    "        source = r.data\n",
    "        \n",
    "            # parse them into lxml tree\n",
    "        tree = html.document_fromstring(source) \n",
    "        \n",
    "        # <Filtering out, non-existent url, years > 2018, non-movie)  \n",
    "        \n",
    "        # 2). pre save indicator\n",
    "        #year_elem = tree.cssselect('div div div div div h1 span#titleYear')\n",
    "        \n",
    "            # 1). non-movie filter!\n",
    "        if ('Gross USA:' not in source): #or (len(year_elem) == 0) or (int([k for k in [i.text_content() for i in year_elem]][0].strip('()')) > 2018)\n",
    "            print('{} non-movie'.format(iter))\n",
    "            continue\n",
    "            \n",
    "        #if len(year_elem) == 0 :\n",
    "        #    print('{} no year info'.format(iter))\n",
    "        #    continue\n",
    "        \n",
    "        #if int([k for k in [i.text_content() for i in year_elem]][0].strip('()')) > 2018:\n",
    "        #    print('loss at {} error while year comparison'.format(iter))\n",
    "        #    continue\n",
    "            \n",
    "        # <Get infoS>\n",
    "            # find a movie name => 'title'\n",
    "        title_str = [i.text_content().encode('utf-8') for i in tree.cssselect('div div div div div h1')][0]\n",
    "        title = title_str.split('\\xc2')[0]\n",
    "            # find a movie star score => 'score'\n",
    "        score = float([i.text_content() for i in tree.cssselect('div div div div div div strong span')][0])\n",
    "            # find a us_gross_box_office_revenue => 'us_gross'\n",
    "        gross_str = [i.text_content() for i in tree.cssselect('#main_bottom div#titleDetails.article')][0].encode('utf-8')\n",
    "        us_gross = int([i for i in gross_str.split('\\n') if i.startswith('Gross USA:')][0].split('$')[-1].replace(',',''))\n",
    "        \n",
    "        # Get metalink\n",
    "        # Get html of the metalink\n",
    "        # find /~~? part to to use as a key into review page.\n",
    "        # integrate the found ~~ part with \n",
    "        # f'https://www.metacritic.com/movie/{john-wick}/critic-reviews' and get that url html\n",
    "        # case : 1. data exist : crawl\n",
    "        \n",
    "        # find the piece\n",
    "        first_piece = [link[2] for link in tree.cssselect('div div div div.titleReviewBarSubItem div a')[0].iterlinks()][0]\n",
    "        \n",
    "        # make 2nd url of l (now link)\n",
    "        new_list = l.split('/')[:-1]\n",
    "        new_list.append(first_piece)\n",
    "        second_url = '/'.join(new_list)\n",
    "        \n",
    "        # get html of 2nd url\n",
    "        r = http.request('GET', second_url)\n",
    "        source = r.data\n",
    "        # and parse it\n",
    "        tree = html.document_fromstring(source)\n",
    "        # get /~? => matacritic's title from it.\n",
    "        clue_string = [i for i in tree.cssselect('div div.see-more')[0].iterlinks()][0][2].split('?')[0].split('/')[-1]\n",
    "        # get 3rd url out of clue string\n",
    "        all_review_url = 'https://www.metacritic.com/movie/{}/critic-reviews'.format(clue_string)\n",
    "        \n",
    "        #<in the all_review_url> - make a list of scores and a list of texts.\n",
    "        #    get html of all_review_url\n",
    "        r = http.request('GET', all_review_url)\n",
    "        source = r.data\n",
    "        tree = html.document_fromstring(source)\n",
    "        \n",
    "        #     get a list of critic scores\n",
    "        lists_of_each_text_list = []\n",
    "        list_of_critic_scores = []\n",
    "\n",
    "        for i in tree.cssselect('div.left.fl'):\n",
    "            lists_of_each_text_list.append(i.text_content())\n",
    "\n",
    "        for k in range(len(lists_of_each_text_list)):\n",
    "            list_of_critic_scores.append(int(lists_of_each_text_list[k].split('\\n')[4]))\n",
    "        #<list_of_critic_scores> completed!!\n",
    "        \n",
    "        #     get a list of critic texts\n",
    "        a_movie_critic_texts_list = []\n",
    "        for j in tree.cssselect('div div div div div div div div a.no_hover'):\n",
    "            a_critic_text = j.text_content().strip().encode('utf-8')\n",
    "            a_movie_critic_texts_list.append(a_critic_text)\n",
    "    \n",
    "        #<a_movie_critic_texts_list> completed\n",
    "        \n",
    "        #write line by line into csv file\n",
    "        \n",
    "        for c in range(len(list_of_critic_scores)):\n",
    "            wr.writerow([title, score, us_gross,list_of_critic_scores[c],a_movie_critic_texts_list[c]])\n",
    "            \n",
    "    # if it's another case!        \n",
    "    except Exception as e:\n",
    "        print(\"unknown error at url index {}\".format(iter)) # ignore if there are already so many data.\n",
    "        continue\n",
    "        \n",
    "# Close csv.file\n",
    "f.close()\n",
    "\n",
    "# Tell me\n",
    "os.system('say \"your program has been completed.\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_2]",
   "language": "python",
   "name": "conda-env-python_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
